{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzTMafxT_IlY"
      },
      "source": [
        "# Discovery of Musical Patterns and Structures using Mathematical Morphology\n",
        "\n",
        "<div style=\"display: flex; justify-content: space-between; align-items: center; padding: 20px 0;\">\n",
        "    <img src=\"https://imgs.search.brave.com/xY8R_8l4B2YioiaJAn-_C_rdzuVm0GxAbIF6mx9uglE/rs:fit:500:0:0:0/g:ce/aHR0cHM6Ly9pcm1p/YXBwLnVuaXN0cmEu/ZnIvd2Vic2l0ZXMv/SVRJLUlSTUlBcHAv/TG9nb3MvbG9nb19p/cm1hLnBuZw\" style=\"height: 80px;\">\n",
        "    <img src=\"https://imgs.search.brave.com/abpAYumpwZ7KG6ngCp5KWlZNqAdbOjBuAe13-lX2awg/rs:fit:500:0:0:0/g:ce/aHR0cHM6Ly93d3cu/dW5pc3RyYS5mci9m/aWxlYWRtaW4vdGVt/cGxhdGVzL3VuaXN0/cmEtdjMvdGVtcGxh/dGVzL2ltYWdlcy93/ZWItZXQtYXNzb2Np/ZS1OQi5wbmc\" style=\"height: 80px;\">\n",
        "</div>\n",
        "\n",
        "**Author:** Mathys DANIEL\n",
        "\n",
        "**Based on the work of:** Paul LASCABETTES (Thesis: [https://hal.science/tel-04480227](https://hal.science/tel-04480227))\n",
        "\n",
        "## Project Description\n",
        "\n",
        "**Objective:** To develop a fully automatic program that captures musical patterns and deduces time signatures for individual tracks and instruments using exclusively topological tools, achieving greater computational efficiency than state-of-the-art methods.\n",
        "\n",
        "**Methodology:** We will leverage mathematical morphology to analyze the inherent structure of musical data, identifying recurring motifs and rhythmic variations.\n",
        "\n",
        "**Expected Outcome:** To demonstrate the program’s ability to handle challenging rhythmic structures and extract meaningful musical information with minimal computational overhead.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4585mKJlj2s"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install with proper versions\n",
        "!python -m pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "2YtrCAkdfwar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8bEmAWtlbp0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import math\n",
        "from math import isclose, modf\n",
        "from fractions import Fraction\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict, Callable\n",
        "\n",
        "# Scientific stack\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "import matplotlib.ticker as mticker\n",
        "from matplotlib.ticker import MultipleLocator\n",
        "import matplotlib.colors as mcolors\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# MIDI processing\n",
        "import pretty_midi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEVnSGY3nWtj"
      },
      "source": [
        "# Topological Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section defines some topological tools that are useful for our algorithms."
      ],
      "metadata": {
        "id": "Timod4Jsvz6i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6tbjfjcnaIH"
      },
      "outputs": [],
      "source": [
        "def erosion(X, S):\n",
        "    \"\"\"\n",
        "    Compute the erosion of X by structuring element S, with S centered at the origin.\n",
        "    \"\"\"\n",
        "    # Get the first point of S to determine the translation vector\n",
        "    s0, s1 = S[0]\n",
        "\n",
        "    # Translate S to be centered at the origin\n",
        "    S_centered = [(so - s0, sp - s1) for (so, sp) in S]\n",
        "\n",
        "    eroded = list()\n",
        "\n",
        "    X = set(X)\n",
        "\n",
        "    for (o, p) in X:\n",
        "        # Check if translated structuring element is included in X\n",
        "        if all((o + so, p + sp) in X for (so, sp) in S_centered):\n",
        "            eroded.append((o, p))\n",
        "\n",
        "    return eroded\n",
        "\n",
        "def dilation(X, S):\n",
        "    \"\"\"\n",
        "    Compute the dilation of set X by structuring element S.\n",
        "    \"\"\"\n",
        "    dilated = list()\n",
        "\n",
        "    X = set(X)\n",
        "\n",
        "    # Get the first point of S to determine the translation vector\n",
        "    s0, s1 = S[0]\n",
        "\n",
        "    # Translate S to be centered at the origin\n",
        "    S_centered = [(so - s0, sp - s1) for (so, sp) in S]\n",
        "\n",
        "    for (o, p) in X:\n",
        "        for (so, sp) in S_centered:\n",
        "            dilated.append((o + so, p + sp))\n",
        "\n",
        "    return dilated\n",
        "\n",
        "# opening (erosion then dilation)\n",
        "def opening(X, S):\n",
        "    \"\"\"\n",
        "    Compute the opening of set X by structuring element S.\n",
        "    \"\"\"\n",
        "    return dilation(erosion(X, S), S)\n",
        "\n",
        "# morphological closing of a set X (dilation then erosion)\n",
        "def closing(X, S):\n",
        "    \"\"\"\n",
        "    Compute the closing of set X by structuring element S.\n",
        "    \"\"\"\n",
        "    return erosion(dilation(X, S), S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn5ky-fUnmYr"
      },
      "source": [
        "# Metric Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section defines the algorithm that detects the patterns in a given piece of music."
      ],
      "metadata": {
        "id": "KV2v8EcmP90V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaGf9IcO0cOn"
      },
      "outputs": [],
      "source": [
        "def windowing(P, L):\n",
        "    \"\"\"\n",
        "    Splits the pattern P into multiple sub-patterns of exact length L.\n",
        "    - Ensures each window starts at a valid onset.\n",
        "    - Prevents empty windows.\n",
        "    \"\"\"\n",
        "    if not P:\n",
        "      return []\n",
        "\n",
        "    P = sorted(P, key=lambda x: x[0])  # Sort by onset\n",
        "    windowed_patterns = []\n",
        "    i = 0  # Index to track starting point\n",
        "\n",
        "    while i < len(P):\n",
        "        t1 = P[i][0]  # Base onset for this window\n",
        "        window = [p for p in P if t1 <= p[0] < t1 + L]\n",
        "\n",
        "        if not window:  # Skip empty windows\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        windowed_patterns.append(window)\n",
        "\n",
        "        # Move to next onset after the current window\n",
        "        while i < len(P) and P[i][0] < t1 + L:\n",
        "            i += 1\n",
        "\n",
        "    return windowed_patterns\n",
        "\n",
        "def filtering(P_list, P_full, L, Nmin=4):\n",
        "    \"\"\"\n",
        "    Filter and merge the subpatterns that repeats periodically inside the previously found patterns\n",
        "    \"\"\"\n",
        "    if not P_list:\n",
        "        return []\n",
        "\n",
        "    P_to_test = [tuple(map(tuple, Pj)) for Pj in P_list]\n",
        "    P_output = []\n",
        "    P_all = P_to_test.copy()\n",
        "    P_to_test = sorted(P_to_test, key=lambda pat: min(p[0] for p in pat))\n",
        "\n",
        "    while P_to_test:\n",
        "        Pj = P_to_test.pop(0)\n",
        "        Pj_list = list(Pj)\n",
        "        onset_j = min(p[0] for p in Pj_list)\n",
        "        opening_Pj = opening(P_full, Pj_list)\n",
        "\n",
        "        Nj = Nmin\n",
        "        absorbed = []\n",
        "\n",
        "        # This list tracks all valid successive window onsets relative to Pj\n",
        "        reference_onsets = [onset_j]\n",
        "\n",
        "        for Pk in P_all:\n",
        "            if Pk == Pj or Pk not in P_to_test:\n",
        "                continue\n",
        "\n",
        "            Pk_list = list(Pk)\n",
        "            onset_k = min(p[0] for p in Pk_list)\n",
        "\n",
        "            if any(p in opening_Pj for p in Pk_list):\n",
        "                # Check if onset_k is in a window immediately after one of the reference windows\n",
        "                window_diff_ok = any(round((onset_k - ref_onset) / L) == 1 for ref_onset in reference_onsets)\n",
        "\n",
        "                if window_diff_ok:\n",
        "                    Nj += 1\n",
        "                    if all(p in opening_Pj for p in Pk_list):\n",
        "                        absorbed.append(Pk)\n",
        "                        reference_onsets.append(onset_k)  # Extend the chain!\n",
        "\n",
        "        for Pk in absorbed:\n",
        "            if Pk in P_to_test:\n",
        "                P_to_test.remove(Pk)\n",
        "\n",
        "        P_output.append((Pj_list, Nj))\n",
        "\n",
        "    return P_output\n",
        "\n",
        "\n",
        "def pattern_detection(X, Lmax = 20, Nmin=4):\n",
        "    \"\"\"\n",
        "    Extracts translation-invariant patterns from input list X for various L values.\n",
        "    - Applies erosion to detect patterns.\n",
        "    - Uses windowing to extract patterns of exact length L.\n",
        "    - Uses filtering to refine extracted patterns.\n",
        "    \"\"\"\n",
        "    found_patterns = []\n",
        "    L_values = np.arange(2, Lmax, 0.25)\n",
        "\n",
        "    for L in L_values:\n",
        "        # Step 1: Define structuring element O_L as a list\n",
        "        O_L = [(n*L, 0) for n in range(Nmin)]\n",
        "\n",
        "        # Step 2: Apply erosion to find patterns\n",
        "        P = erosion(X, O_L)\n",
        "\n",
        "        # Step 3: Windowing to get patterns of exact length L\n",
        "        P1_to_Pn = windowing(P, L)\n",
        "\n",
        "        # Step 4: Filtering to refine patterns\n",
        "        filtered_patterns = filtering(P1_to_Pn, P, L, Nmin)\n",
        "\n",
        "        # Step 5: Store results\n",
        "        for P_prime, N in filtered_patterns:\n",
        "            found_patterns.append((P_prime, N, float(L)))\n",
        "\n",
        "    return found_patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5ZeZAdmiaqJ"
      },
      "source": [
        "# Metric visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVTOkbJ935wQ"
      },
      "source": [
        "### Ground Truth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section extracts time signature from dataset info"
      ],
      "metadata": {
        "id": "4c-g5fKaS-aX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GT6twHQE_VBG"
      },
      "outputs": [],
      "source": [
        "def _parse_ts_string(ts_string):\n",
        "    \"\"\"\n",
        "    '7/8 @ 0.000 beats; 6/4 @ 406.000 beats'\n",
        "        -> [{'beat':0.0,'num':7,'den':8}, …]\n",
        "    \"\"\"\n",
        "    if not ts_string:\n",
        "        return []\n",
        "    events = []\n",
        "    for chunk in ts_string.split(';'):\n",
        "        m = re.match(r'\\s*(\\d+)\\s*/\\s*(\\d+)\\s*@\\s*([0-9.]+)', chunk)\n",
        "        if m:\n",
        "            num, den, beat = int(m[1]), int(m[2]), float(m[3])\n",
        "            events.append({'beat': beat, 'num': num, 'den': den})\n",
        "    return sorted(events, key=lambda d: d['beat'])\n",
        "# ------------------------------------------------- ①\n",
        "\n",
        "def _format_time_signature_in_beats(ts_obj, pm):\n",
        "    \"\"\"\n",
        "    Returns the time signature in beats.\n",
        "    \"\"\"\n",
        "    # Convert absolute seconds → ticks → beats\n",
        "    beat_idx = pm.time_to_tick(ts_obj.time) / pm.resolution\n",
        "    return f\"{ts_obj.numerator}/{ts_obj.denominator} @ {beat_idx:.3f} beats\"\n",
        "\n",
        "\n",
        "def check_time_signature_presence(folder_path, show_table = False):\n",
        "    \"\"\"\n",
        "    Scan every MIDI file in folder_path, report whether it contains\n",
        "    Time-Signature events, how many, and list them.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "\n",
        "    for fname in os.listdir(folder_path):\n",
        "        if not fname.lower().endswith((\".mid\", \".midi\")):\n",
        "            continue\n",
        "\n",
        "        path = os.path.join(folder_path, fname)\n",
        "        try:\n",
        "            pm = pretty_midi.PrettyMIDI(path)\n",
        "            ts_changes = pm.time_signature_changes                   # list[TimeSignature]\n",
        "            n_ts       = len(ts_changes)\n",
        "\n",
        "            # ---------- tempo information ---------------------------\n",
        "            tempo_changes, tempi = pm.get_tempo_changes()\n",
        "            bpm = float(tempi[0]) if len(tempi) else 120.0\n",
        "\n",
        "            ts_strs = [_format_time_signature_in_beats(ts, pm) for ts in ts_changes]    # turn each TimeSignature into a readable string (in beats)\n",
        "\n",
        "\n",
        "            rows.append({\n",
        "                \"midi_file\"                 : fname,\n",
        "                \"bpm\"                       : bpm,\n",
        "                \"has_time_signature\"        : bool(n_ts),\n",
        "                \"num_time_signature_events\" : n_ts,\n",
        "                \"time_signatures\"           : \"; \".join(ts_strs) if ts_strs else \"\"\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Could not parse {fname}: {e}\")\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    # Print the table with all columns\n",
        "    if show_table:\n",
        "      if not df.empty:\n",
        "            print(df.to_string(index=False))\n",
        "      else:\n",
        "          print(\"No MIDI files found.\")\n",
        "\n",
        "      # Global summary\n",
        "      if df[\"has_time_signature\"].any():\n",
        "          print(\"\\n✓  At least one file contains explicit time-signature data.\")\n",
        "      else:\n",
        "          print(\"\\n✗  No file in the folder contains any time-signature events.\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VQMpR2G3-t0"
      },
      "source": [
        "### Plot sections"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section defines all the tools and apply them to visualize the metric construction of a given piece of music, using the algorithm previously defined."
      ],
      "metadata": {
        "id": "pQeZCpJHTXzb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJ1MzcI1Sa9X"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "#  0) Figure parameters definition\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "mpl.rcParams.update({\n",
        "    # --- fonts -----------------------------------------------------\n",
        "    'font.family'       : 'serif',\n",
        "    'font.serif'        : ['Computer Modern Roman', 'cmr10'],\n",
        "    'mathtext.fontset'  : 'cm',\n",
        "    'font.size'         : 11,          # base text size\n",
        "    'axes.titlesize'    : 14,          # title → a bit larger\n",
        "    'axes.titleweight'  : 'bold',\n",
        "\n",
        "    # --- axes & spines --------------------------------------------\n",
        "    'axes.edgecolor'    : '#333333',\n",
        "    'axes.linewidth'    : .8,\n",
        "    'axes.grid'         : False,       # we enable it manually, x–only\n",
        "    'xtick.direction'   : 'out',\n",
        "    'ytick.direction'   : 'out',\n",
        "\n",
        "    # --- legend ----------------------------------------------------\n",
        "    'legend.frameon'    : False,\n",
        "    'legend.fontsize'   : 9,\n",
        "\n",
        "    # --- figure / saving ------------------------------------------\n",
        "    'figure.dpi'        : 150,\n",
        "    'savefig.dpi'       : 300,\n",
        "    'figure.autolayout' : True         # keeps labels from colliding\n",
        "})\n",
        "\n",
        "# ---------- SECTIONS COLOR MAPPING ---------------------------\n",
        "PRESET_BASE_COLORS = {\n",
        "     2 : \"#bcbd22\",   # olive (close to green, because it's musically close to 4)\n",
        "     3 : \"#FF0000\",   # red\n",
        "     4 : \"#2ca02c\",   # green\n",
        "     5 : \"#ff7f0e\",   # orange\n",
        "     7 : \"#6a3d9a\",   # purple\n",
        "     9 : \"#17becf\",   # teal (separated from 3, because 9 is often not used as 3x3 but as a whole 9)\n",
        "    11 : \"#8c564b\",   # brown\n",
        "    13 : \"#E30B5C\",   # pink\n",
        "    17 : \"#008080\",   # teal\n",
        "}\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "#  1)  Low-level numeric helpers\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def group_multiplicative_bases(Ls: List[float]) -> List[List[float]]:\n",
        "    \"\"\"\n",
        "    Group L values by shared multiplicative base.\n",
        "\n",
        "    If multiple base candidates are valid (i.e., L is divisible by several others),\n",
        "    we assign it to the group with the **largest** such base.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    base_groups : list of groups, each group being a list of Ls.\n",
        "    \"\"\"\n",
        "    base_groups = []\n",
        "    base_lookup = {}  # maps base -> group\n",
        "\n",
        "    for L in sorted(Ls):\n",
        "        best_base = None\n",
        "        for base in base_lookup:\n",
        "            ratio = L / base\n",
        "            ratio_inv = base / L\n",
        "            if (isclose(ratio, round(ratio), abs_tol=1e-8) and ratio >= 1) or \\\n",
        "               (isclose(ratio_inv, round(ratio_inv), abs_tol=1e-8) and ratio_inv >= 1):\n",
        "                if best_base is None or base > best_base:\n",
        "                    best_base = base\n",
        "        if best_base is not None:\n",
        "            base_lookup[best_base].append(L)\n",
        "        else:\n",
        "            base_lookup[L] = [L]\n",
        "\n",
        "    return list(base_lookup.values())\n",
        "\n",
        "\n",
        "def assign_vertical_layers(x_positions: List[float], min_gap: float) -> List[int]:\n",
        "    \"\"\"\n",
        "    Greedy layer assignment so that labels (placed at each x_positions)\n",
        "    do not overlap horizontally by less than min_gap.\n",
        "\n",
        "    Returns a list `layer_index`  (same length as x_positions).\n",
        "    \"\"\"\n",
        "    layers, label_layers = [], []          # running accumulation\n",
        "    for x in x_positions:\n",
        "        # Try to squeeze into an existing layer\n",
        "        for i, layer in enumerate(layers):\n",
        "            if all(abs(x - lx) > min_gap for lx in layer):\n",
        "                layer.append(x)\n",
        "                label_layers.append(i)\n",
        "                break\n",
        "        else:\n",
        "            # No layer was available → create a new one\n",
        "            layers.append([x])\n",
        "            label_layers.append(len(layers) - 1)\n",
        "    return label_layers\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "#  2)  Pattern / compactness computation\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def compute_compactness_segments_for_instrument(\n",
        "        points: List[Tuple[float, int]],\n",
        "        pattern_detection: Callable,\n",
        "        dilation: Callable[[list, list], list],\n",
        "        global_id_start: int,\n",
        "        Lmax: int\n",
        ") -> Tuple[List[Dict], Dict[int, Dict], int]:\n",
        "    \"\"\"\n",
        "    For one instrument track:\n",
        "    • Call pattern_detection to get repeating patterns.\n",
        "    • Dilate each pattern (so repetitions overlay each other).\n",
        "    • Compute a 'compactness' score.\n",
        "    • Assign a globally unique band ID starting from global_id_start.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bands : list of filtered bands (dicts)\n",
        "    band_map : dict of {id → band}\n",
        "    next_id : next available global ID\n",
        "    \"\"\"\n",
        "    patterns = pattern_detection(points, Lmax=Lmax, Nmin=4)\n",
        "    bands = []\n",
        "    band_map = {}\n",
        "    points_set = set(map(tuple, points))\n",
        "    current_id = global_id_start\n",
        "\n",
        "    for P_i, N_i, L_i in patterns:\n",
        "        if not P_i:\n",
        "            continue\n",
        "\n",
        "        O_L_N = [(n * L_i, 0) for n in range(N_i)]\n",
        "        dilated_i = dilation(P_i, O_L_N)\n",
        "        if not dilated_i:\n",
        "            continue\n",
        "\n",
        "        dilated_set = set(map(tuple, dilated_i))\n",
        "        x_min = min(o for o, _ in dilated_i)\n",
        "        x_max = max(o for o, _ in dilated_i)\n",
        "        pts_in_range = [p for p in points if x_min <= p[0] <= x_max]\n",
        "        pertes = len(pts_in_range) - len(dilated_set)\n",
        "        compactness = len(pts_in_range) / (len(P_i) + 2 + pertes)\n",
        "\n",
        "        band = dict(\n",
        "            id=current_id,\n",
        "            L=L_i, N=N_i, compactness=compactness,\n",
        "            pattern_len=len(P_i),\n",
        "            min_on=x_min, max_on=x_max,\n",
        "            dilated_set=dilated_set,\n",
        "            original_points_set=points_set\n",
        "        )\n",
        "        bands.append(band)\n",
        "        band_map[current_id] = band\n",
        "        current_id += 1\n",
        "\n",
        "    filtered = refine_overlapping_segments(bands)\n",
        "    return filtered, band_map, current_id\n",
        "\n",
        "\n",
        "def refine_overlapping_segments(\n",
        "        bands: List[Dict],\n",
        "        max_iterations: int = 5 # chosen arbitrarily for computational purposes\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Resolve overlaps between candidate bands, keeping the most compact one.\n",
        "\n",
        "    Compactness is re-evaluated after removing shared points; the process is\n",
        "    iterated until no further changes occur or max_iterations hits.\n",
        "    \"\"\"\n",
        "    bands.sort(key=lambda b: b['min_on'])\n",
        "    for _ in range(max_iterations):\n",
        "        changed = False\n",
        "        for i, b1 in enumerate(bands):\n",
        "            for j in range(i + 1, len(bands)):\n",
        "                b2 = bands[j]\n",
        "                if b2['min_on'] > b1['max_on']:   # no horizontal overlap → break\n",
        "                    break\n",
        "                if b1['max_on'] >= b2['min_on']:  # overlap exists\n",
        "                    winner, loser = (b1, b2) if b1['compactness'] > b2['compactness'] else (b2, b1)\n",
        "                    overlap = loser['dilated_set'] & winner['dilated_set']\n",
        "                    if overlap:\n",
        "                        # Remove shared points from the loser\n",
        "                        loser['dilated_set'] -= overlap\n",
        "                        if not loser['dilated_set']:\n",
        "                            loser['compactness'] = float('inf')\n",
        "                            changed = True\n",
        "\n",
        "                        else:\n",
        "                            x_min = min(p[0] for p in loser['dilated_set'])\n",
        "                            x_max = max(p[0] for p in loser['dilated_set'])\n",
        "\n",
        "                            pts_in_range = [p for p in loser['original_points_set']\n",
        "                                            if x_min <= p[0] <= x_max]\n",
        "                            pertes = len(pts_in_range) - len(loser['dilated_set'])\n",
        "                            if  (loser['pattern_len'] + 2 + pertes) <= 0:\n",
        "                              loser['compactness'] = float('inf')\n",
        "                              continue\n",
        "                            # Update the loser's range and compactness\n",
        "                            loser.update(min_on=x_min,\n",
        "                                        max_on=x_max,\n",
        "                                        compactness=len(pts_in_range) / (loser['pattern_len'] + 2 + pertes))\n",
        "                            changed = True\n",
        "        # Purge invalidated losers\n",
        "        bands = [b for b in bands if b['compactness'] != float('inf')]\n",
        "        if not changed:\n",
        "            break\n",
        "    return bands\n",
        "\n",
        "def compute_compactness_segments_for_all_instruments(\n",
        "        track_instruments: Dict[str, list],\n",
        "        pattern_detection: Callable,\n",
        "        dilation: Callable,\n",
        "        Lmax: int\n",
        ") -> Tuple[Dict[str, List[Dict]], Dict[int, Dict]]:\n",
        "    \"\"\"\n",
        "    For all instrument tracks:\n",
        "    • Call compute_compactness_segments_for_instrument with global IDs.\n",
        "    • Collect filtered bands per instrument.\n",
        "    • Return global band map and per-instrument bands.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    all_bands : dict[str → list of band dicts]\n",
        "    band_map  : dict[id → band dict]\n",
        "    \"\"\"\n",
        "    all_bands = {}\n",
        "    band_map = {}\n",
        "    global_id = 0\n",
        "\n",
        "    for inst, points in track_instruments.items():\n",
        "        filtered, band_dict, global_id = compute_compactness_segments_for_instrument(points, pattern_detection, dilation, global_id, Lmax)\n",
        "        all_bands[inst] = filtered\n",
        "        band_map.update(band_dict)\n",
        "\n",
        "    return all_bands, band_map\n",
        "\n",
        "def extract_best_compactness_and_merge_intervals(\n",
        "        bands: List[Dict]\n",
        ") -> Tuple[List[Dict], List[Dict]]:\n",
        "    \"\"\"\n",
        "    Extract the best-scoring (biggest compactness) segment for every elementary\n",
        "    sub-interval found in bands.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    exhaustive_intervals : List[Dict]\n",
        "        Complete list of all non-overlapping intervals (one per elementary segment)\n",
        "    merged_intervals : List[Dict]\n",
        "        Merged contiguous segments for display purposes only\n",
        "    \"\"\"\n",
        "    # ➊  Obtain all cut points\n",
        "    cps = sorted({pt for b in bands for pt in (b['min_on'], b['max_on'])})\n",
        "    exhaustive_intervals = []\n",
        "\n",
        "    # ➋  For each elementary (a, b) choose the best band covering it\n",
        "    for a, b in zip(cps, cps[1:]):\n",
        "        cand = [x for x in bands if a < x['max_on'] and b > x['min_on']]\n",
        "        if cand:\n",
        "            best = max(cand, key=lambda x: (x['compactness'],  x['N'],  x['L']))\n",
        "            exhaustive_intervals.append({'band_id':  best['id'],\n",
        "                              'L': best['L'],\n",
        "                              'compactness': best['compactness'],\n",
        "                              'a': a, 'b': b})\n",
        "\n",
        "    # Keep only merged segments whose length ≥ one full L\n",
        "    exhaustive_intervals = [s for s in exhaustive_intervals if (s['b'] - s['a']) >= s['L']]\n",
        "\n",
        "    # ➌  Create merged version for display (merge consecutive segments with same L)\n",
        "    merged_intervals = []\n",
        "\n",
        "    # Sort segments by start position\n",
        "    sorted_segments = sorted(exhaustive_intervals, key=lambda x: x['a'])\n",
        "\n",
        "    for seg in sorted_segments:\n",
        "        # Check if we can merge with the last segment in merged_intervals\n",
        "        if (merged_intervals and\n",
        "            isclose(seg['L'], merged_intervals[-1]['L']) and\n",
        "            # Check if there's no other segment between the current and the last merged one\n",
        "            not any(other_seg['b'] > merged_intervals[-1]['b'] and other_seg['a'] < seg['a']\n",
        "                   for other_seg in sorted_segments\n",
        "                   if not isclose(other_seg['L'], seg['L']))):\n",
        "\n",
        "            # Extend the last merged segment\n",
        "            merged_intervals[-1]['b'] = seg['b']\n",
        "            # Update compactness to the maximum of the merged segments\n",
        "            merged_intervals[-1]['compactness'] = max(merged_intervals[-1]['compactness'], seg['compactness'])\n",
        "        else:\n",
        "            # Create new merged segment (copy of current segment)\n",
        "            merged_intervals.append(seg.copy())\n",
        "\n",
        "    return exhaustive_intervals, merged_intervals\n",
        "\n",
        "\n",
        "def compute_global_compactness(track_instruments: Dict[str, list],\n",
        "                               bands_by_inst: Dict[str, List[Dict]]) -> float:\n",
        "    \"\"\"\n",
        "    Global score\n",
        "\n",
        "        GC = |P_tot| /\n",
        "             ( Σ_patterns (len(pattern) + 2)  +  |misses| )\n",
        "\n",
        "    • P_tot      : set of every original (time, pitch) point\n",
        "    • len(pattern): stored in each band as  b['pattern_len']\n",
        "    • misses     : original points that do **not** belong to any\n",
        "                   dilated_set of the retained patterns\n",
        "    \"\"\"\n",
        "    # ➊  All original points ---------------------------------------\n",
        "    all_points = {\n",
        "        tuple(p)\n",
        "        for pts in track_instruments.values()\n",
        "        for p   in pts\n",
        "    }\n",
        "\n",
        "    # ➋  Union of all dilated points and the pattern-cost term -----\n",
        "    covered_points   = set()\n",
        "    penalty_patterns = 0          # Σ (len(pattern) + 2)\n",
        "    n_motifs = 0\n",
        "\n",
        "    for bands in bands_by_inst.values():\n",
        "        for b in bands:\n",
        "            covered_points |= b['dilated_set']\n",
        "            penalty_patterns += (b['pattern_len']*2 + 2)\n",
        "            n_motifs += 1\n",
        "\n",
        "    # ➌  Points not covered by any pattern -------------------------\n",
        "    misses = all_points - covered_points\n",
        "\n",
        "    # ➍  Final compactness -----------------------------------------\n",
        "    print(f\"Pourcentage de points perdus : {100 * len(misses) / len(all_points):.2f} %\")\n",
        "    print(\"Nombre de motifs:\", n_motifs)\n",
        "\n",
        "    return len(all_points)*2 / (penalty_patterns + len(misses)*2)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "#  3)  Colour assignment for metric groups\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def assign_color_groups_by_multiplicative_base(all_Ls: List[float]):\n",
        "    \"\"\"\n",
        "    Create a colour map for each encountered L value.\n",
        "\n",
        "    • All lengths are grouped by the smallest integer 'base' such that\n",
        "      they differ by an integer ratio; each group gets its own colour.\n",
        "\n",
        "    If a base is not found in the mapping dictionary (PRESET_BASE_COLORS)\n",
        "    and has no multiplicative relation to one that is, it gets default grey.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Build multiplicative groups\n",
        "    base_groups = group_multiplicative_bases(all_Ls)\n",
        "\n",
        "    # 2) Label each group by its smallest integer (if any)\n",
        "    base_labels = {}\n",
        "    for group in base_groups:\n",
        "        int_members = [x for x in group if isclose(x, round(x), abs_tol=1e-8)]\n",
        "        label = int(min(int_members)) if int_members else min(group)\n",
        "        for L in group:\n",
        "            base_labels[L] = label\n",
        "\n",
        "    # Special case for L=2 (because it's difficult to handle, half the integer values would be in its base)\n",
        "    if any(isclose(L, 2.0, abs_tol=1e-8) for L in all_Ls):\n",
        "        base_labels[2.0] = 2\n",
        "\n",
        "    # 3) Color each base label\n",
        "    color_for_base = {\n",
        "        b: mcolors.to_rgba(c) for b, c in PRESET_BASE_COLORS.items()\n",
        "    }\n",
        "\n",
        "    known_bases = set(PRESET_BASE_COLORS.keys()) - {2}\n",
        "\n",
        "    def find_related_base(base, knowns):\n",
        "        for known in knowns:\n",
        "            ratio = base / known\n",
        "            ratio_inv = known / base\n",
        "            if (isclose(ratio, round(ratio), abs_tol=1e-8) and ratio >= 1) or \\\n",
        "               (isclose(ratio_inv, round(ratio_inv), abs_tol=1e-8) and ratio_inv >= 1):\n",
        "                return known\n",
        "        return None\n",
        "\n",
        "    for base in sorted({base_labels[L] for L in base_labels}):\n",
        "        if base in color_for_base:\n",
        "            continue\n",
        "        related = find_related_base(base, known_bases)\n",
        "        if related and related in color_for_base:           # in a mapped base\n",
        "            color_for_base[base] = color_for_base[related]\n",
        "        else:                                               # not in a mapped base\n",
        "            color_for_base[base] = (0.6, 0.6, 0.6, 0.5)  # Default grey\n",
        "\n",
        "    # 4) Final L → RGBA assignment\n",
        "    color_for_L = {}\n",
        "    for L in all_Ls:\n",
        "        base = base_labels.get(L, L)\n",
        "        color_for_L[L] = color_for_base[base]\n",
        "\n",
        "    return color_for_L, base_labels, color_for_base\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "#  4)  Plotting helpers\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def plot_instrument_onsets(ax, track_instruments, instrument_to_y):\n",
        "    \"\"\"\n",
        "    Scatter-plot every individual onset for every instrument.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ax : matplotlib-axes\n",
        "        The axes on which everything is drawn.\n",
        "\n",
        "    track_instruments : dict[str, list[(float,int)]]\n",
        "        Maps an instrument-name to a list of tuples (onsetBeat, midiPitch).\n",
        "\n",
        "    instrument_to_y : dict[str, int]\n",
        "        Pre-computed mapping  instrument → y-row index.\n",
        "    \"\"\"\n",
        "    for inst, pts in track_instruments.items():\n",
        "        y = instrument_to_y[inst]\n",
        "        ax.scatter(\n",
        "            [float(o) for o, _ in pts],   # x-coordinates (beats), but final output will be in seconds\n",
        "            [y] * len(pts),               # y-coordinates (fixed row)\n",
        "            s=10, color='black', alpha=.3\n",
        "        )\n",
        "\n",
        "\n",
        "def plot_segments_with_stacked_labels(ax, intervals_by_inst,\n",
        "                                      instrument_to_y, color_for_L):\n",
        "    \"\"\"\n",
        "    Draw the coloured metric segments (rectangles) and print L values\n",
        "    stacked above them while avoiding label collisions.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    intervals_by_inst : dict[str, list[dict]]\n",
        "        Output of extract_and_merge_best_compactness_intervals per instrument.\n",
        "\n",
        "    color_for_L : dict[float, rgba]\n",
        "        Mapping from every observed L value to its display colour.\n",
        "    \"\"\"\n",
        "    vertical_spacing = .18                       # distance between stacked labels\n",
        "\n",
        "    for inst, merged in intervals_by_inst.items():\n",
        "        y = instrument_to_y[inst]\n",
        "        if not merged:\n",
        "            continue\n",
        "\n",
        "        # ➊  Compute label centres and layer organisation (collision-avoiding)\n",
        "        xcent = [(s['a'] + s['b']) / 2 for s in merged]\n",
        "        order = sorted(range(len(xcent)), key=lambda i: xcent[i])\n",
        "        min_gap = .03 * (ax.get_xlim()[1] - ax.get_xlim()[0] or 1)\n",
        "        layers = assign_vertical_layers([xcent[i] for i in order], min_gap)\n",
        "\n",
        "        layer_idx = [0] * len(merged)\n",
        "        for idx, layer in zip(order, layers):\n",
        "            layer_idx[idx] = layer\n",
        "\n",
        "        # ➋  Actually draw rectangles + labels\n",
        "        for i, s in enumerate(merged):\n",
        "            L = s['L']\n",
        "            if L == int(L):\n",
        "                L_str = str(int(L))\n",
        "            elif L * 10 == int(L * 10):  # ex: 5.5\n",
        "                L_str = f\"{L:.1f}\"\n",
        "            else:\n",
        "                L_str = f\"{L:.2f}\"  # fallback for unusual cases\n",
        "\n",
        "            ax.add_patch(Rectangle((\n",
        "                        s['a'], y - .3),           # (x, y) lower-left\n",
        "                        s['b'] - s['a'], .6,        # width / height\n",
        "                        facecolor=color_for_L.get(s['L'], (0, 0, 0, .1)),\n",
        "                        edgecolor=color_for_L.get(s['L'], (0, 0, 0, .1)),\n",
        "                        alpha=.3))\n",
        "\n",
        "            ax.text((s['a'] + s['b']) / 2,\n",
        "                    y + .15 + layer_idx[i] * vertical_spacing,\n",
        "                    L_str,\n",
        "                    ha='center', va='bottom',\n",
        "                    fontweight='bold', fontsize=10, fontfamily='DejaVu Sans')\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "#  5)  High-level driver: MAIN PLOT (plot_metric_sections_by_best_compactness)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def plot_metric_sections_by_best_compactness(\n",
        "        subset_data: dict,\n",
        "        track_name: str,\n",
        "        pattern_detection, dilation,\n",
        "        ground_truth_df,\n",
        "        Lmax=20):\n",
        "    \"\"\"\n",
        "    Master routine that puts everything together:\n",
        "\n",
        "    • Extract onsets for track_name from subset_data.\n",
        "    • Compute metric candidates, select best by compactness.\n",
        "    • Draw one horizontal strip per instrument plus an extra\n",
        "      'Database Ground Truth' strip.\n",
        "    • Overlay colour-coded rectangles and labels.\n",
        "    • Add a legend grouping L values by their multiplicative base.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    subset_data : dict\n",
        "        {instrument_name → [(onsetBeat, midiPitch), …]}\n",
        "\n",
        "    track_name : str\n",
        "        Name (suffix) of the MIDI file we want to visualise.\n",
        "\n",
        "    algo, dilation : callables\n",
        "        User-supplied routines used earlier for pattern detection / dilation.\n",
        "\n",
        "    ground_truth_df : pandas.DataFrame\n",
        "        Data-frame holding the reference time-signature annotations.\n",
        "\n",
        "    Lmax : int, optional (default=20)\n",
        "        Maximum metric length to consider when searching for patterns.\n",
        "    \"\"\"\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # ➊  Collect all onsets belonging to that MIDI file\n",
        "    # ------------------------------------------------------------------\n",
        "    track_instruments = {k: v for k, v in subset_data.items()\n",
        "                         if k.startswith(track_name)}\n",
        "    if not track_instruments:\n",
        "        print(f\"No data for {track_name}\")\n",
        "        return\n",
        "\n",
        "    # Row mapping (instrument → y-coordinate on the plot)\n",
        "    instruments = sorted(track_instruments)\n",
        "    instrument_to_y = {inst: i for i, inst in enumerate(instruments)}\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # ➋  Analyse compactness for every instrument\n",
        "    # ------------------------------------------------------------------\n",
        "    all_bands, band_map = compute_compactness_segments_for_all_instruments(\n",
        "        track_instruments, pattern_detection, dilation, Lmax)\n",
        "\n",
        "    intervals_by_inst = {}\n",
        "    merged_intervals_by_inst = {}  # Only for display\n",
        "\n",
        "    for inst, band in all_bands.items():\n",
        "        exhaustive_intervals, merged_intervals = extract_best_compactness_and_merge_intervals(band)\n",
        "        if exhaustive_intervals:  # Keep all exhaustive intervals\n",
        "            intervals_by_inst[inst] = exhaustive_intervals\n",
        "            merged_intervals_by_inst[inst] = merged_intervals  # For display\n",
        "\n",
        "    used_ids = {\n",
        "        seg['band_id']\n",
        "        for segs in intervals_by_inst.values()\n",
        "        for seg  in segs}\n",
        "\n",
        "    display_bands = {\n",
        "        inst: [band_map[seg['band_id']] for seg in segs]\n",
        "        for inst, segs in intervals_by_inst.items()}\n",
        "\n",
        "    # Colour bookkeeping\n",
        "    all_L = {s['L'] for v in merged_intervals_by_inst.values() for s in v}\n",
        "    color_for_L, base_for_L, color_for_base = assign_color_groups_by_multiplicative_base(all_L)\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # ➌  Append an extra staff for ground-truth signatures\n",
        "    # ------------------------------------------------------------------\n",
        "    gt_name = \"Database Ground Truth\"\n",
        "    instruments.append(gt_name)\n",
        "    instrument_to_y = {inst: i for i, inst in enumerate(instruments)}\n",
        "    gt_y = instrument_to_y[gt_name]\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # ➍  Prepare figure & draw machine-estimated segments\n",
        "    # ------------------------------------------------------------------\n",
        "    plt.figure(figsize=(16, 1 + .8 * len(instruments)))\n",
        "    ax = plt.gca()\n",
        "\n",
        "    plot_instrument_onsets(ax, track_instruments, instrument_to_y)\n",
        "\n",
        "    plot_segments_with_stacked_labels(ax, merged_intervals_by_inst,\n",
        "                                      instrument_to_y, color_for_L)\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # ➎  Draw ground-truth rectangles (if any) on the bottom staff\n",
        "    # ------------------------------------------------------------------\n",
        "    row = ground_truth_df[ground_truth_df['midi_file']\n",
        "                          .str.endswith(track_name, na=False)]\n",
        "\n",
        "    if not row.empty:\n",
        "        events = _parse_ts_string(row.iloc[0]['time_signatures'])\n",
        "        if events:\n",
        "            x_max = ax.get_xlim()[1]\n",
        "\n",
        "            # Compute centre-points and layer indices for GT labels\n",
        "            gt_centers = []\n",
        "            for i, ev in enumerate(events):\n",
        "                x0 = ev['beat']\n",
        "                x1 = events[i + 1]['beat'] if i + 1 < len(events) else x_max\n",
        "                gt_centers.append((x0 + x1) / 2)\n",
        "\n",
        "            gt_layers = assign_vertical_layers(\n",
        "                gt_centers, min_gap=0.03 * (x_max - 0))\n",
        "\n",
        "            # Actual drawing loop\n",
        "            L_gt_list = []\n",
        "            for i, ev in enumerate(events):\n",
        "                L_gt = ev['num']\n",
        "                L_gt_list.append(L_gt)\n",
        "                x0 = ev['beat']\n",
        "                x1 = events[i + 1]['beat'] if i + 1 < len(events) else x_max\n",
        "                color_for_L_gt, _, _ = assign_color_groups_by_multiplicative_base(L_gt_list)\n",
        "                color = color_for_L_gt.get(L_gt, (0, 0, 0, 0.2))\n",
        "\n",
        "                # Background rectangle\n",
        "                ax.add_patch(Rectangle((x0, gt_y - .3), x1 - x0, .6,\n",
        "                                       facecolor=color, edgecolor= color,\n",
        "                                       alpha=.3, linewidth=.8, zorder=5))\n",
        "\n",
        "                # Time-signature label (e.g. \"7/8\")\n",
        "                y_offset = -0.10 + gt_layers[i] * .18\n",
        "                ax.text((x0 + x1) / 2, gt_y + y_offset,\n",
        "                        f\"{ev['num']}/{ev['den']}\",\n",
        "                        ha='center', va='bottom',\n",
        "                        fontsize=12, fontweight='bold', fontfamily='DejaVu Sans',\n",
        "                        color=color, zorder=6)\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # ➏  Cosmetics: axes, legend, layout\n",
        "    # ------------------------------------------------------------------\n",
        "    ax.set_yticks(list(instrument_to_y.values()))\n",
        "    ax.set_yticklabels([k.split('_')[-1] for k in instruments])\n",
        "\n",
        "    # ------ Convert the beats axis in seconds for visibility purpose -------\n",
        "    track_row = ground_truth_df.loc[\n",
        "        ground_truth_df['midi_file'].str.endswith(track_name, na=False)]\n",
        "\n",
        "    bpm = float(track_row['bpm'].iloc[0]) if not track_row.empty else 120.0\n",
        "    beat_to_sec = 60 / bpm\n",
        "    ax.set_xlabel(\"Temps (s)\")\n",
        "\n",
        "    ax.xaxis.set_major_formatter(mticker.FuncFormatter(lambda beat, pos: f\"{beat*beat_to_sec:.0f}\"))\n",
        "\n",
        "    sec_step   = 60\n",
        "    beat_step  = sec_step / beat_to_sec     # beats per 60 s  (== bpm)\n",
        "    ax.xaxis.set_major_locator(mticker.MultipleLocator(beat_step))     # major ticks every 60s\n",
        "    ax.xaxis.set_minor_locator(mticker.MultipleLocator(beat_step/2))   # minor ticks every 30s\n",
        "    display_name = track_name.removesuffix('.mid')\n",
        "    ax.set_title(f\"{display_name}\")\n",
        "    ax.grid(True, axis='x', ls='--', alpha=.5)\n",
        "    ax.set_xlim(left=0)\n",
        "    ax.set_ylim(-.5, gt_y + .5)\n",
        "\n",
        "    # ---- Legend showing multiplicative bases -------------------------\n",
        "\n",
        "    legend_patches = [mpatches.Patch(color=color_for_base[base],\n",
        "                      label=f\"{base:g}\",\n",
        "                       alpha=0.3)\n",
        "        for base in sorted(PRESET_BASE_COLORS.keys())]\n",
        "\n",
        "    DEFAULT_OTHER_COLOR = (0.6, 0.6, 0.6, .5)\n",
        "    default_patch = mpatches.Patch(color=DEFAULT_OTHER_COLOR,label=\"Autres\")\n",
        "    legend_patches.append(default_patch)\n",
        "\n",
        "    if legend_patches:\n",
        "        ncol = min(len(legend_patches), 8)\n",
        "        ax.legend(handles=legend_patches,\n",
        "                  title=\"Signature rythmique associee\",\n",
        "                  ncol=ncol,\n",
        "                  loc='lower center',\n",
        "                  bbox_to_anchor=(.5, -0.18),\n",
        "                  frameon=True)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    safe_name = track_name.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\".mid\", \"\")\n",
        "    filename = f\"segmentation_{safe_name}.pdf\"\n",
        "    plt.savefig(filename, format='pdf', bbox_inches='tight')\n",
        "    plt.show()\n",
        "    # -------------------\n",
        "    # GLOBAL COMPACTNESS\n",
        "    # -------------------\n",
        "    global_compactness = compute_global_compactness(track_instruments, display_bands)\n",
        "    print(\"Global compactness:\", global_compactness)\n",
        "\n",
        "    return intervals_by_inst, band_map, display_bands, track_instruments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0c0Z_DE9nL0"
      },
      "source": [
        "# To use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUbeKE9UHrq_"
      },
      "source": [
        "## Functionals"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section defines the data preprocessing functions in order to analyze your pieces of music."
      ],
      "metadata": {
        "id": "nu-EUfmLRuj5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekyn5PYIHqZ6"
      },
      "outputs": [],
      "source": [
        "def process_midi_separating_instruments(midi_file_path):\n",
        "    midi_data = pretty_midi.PrettyMIDI(midi_file_path)\n",
        "    tempo_changes, tempos = midi_data.get_tempo_changes()\n",
        "    bpm = tempos[0] if len(tempos) > 0 else Fraction(120)\n",
        "    seconds_to_beats = bpm / 60\n",
        "    tatum = Fraction(1, (8 * 3))\n",
        "    data = []\n",
        "\n",
        "    for instrument in midi_data.instruments:\n",
        "        instrument_name = instrument.name if instrument.name else 'Unknown'\n",
        "        for note in instrument.notes:\n",
        "            onset_in_beats = note.start * seconds_to_beats\n",
        "            duration_in_beats = (note.end - note.start) * seconds_to_beats\n",
        "\n",
        "            rounded_onset = round(onset_in_beats / tatum) * tatum\n",
        "            rounded_duration = round(duration_in_beats / tatum) * tatum\n",
        "\n",
        "            onset_value = float(rounded_onset) if rounded_onset.denominator % 3 != 0 else rounded_onset\n",
        "            duration_value = float(rounded_duration) if rounded_duration.denominator % 3 != 0 else rounded_duration\n",
        "\n",
        "            data.append({\n",
        "                'midi_file': os.path.basename(midi_file_path),\n",
        "                'instrument': instrument_name,\n",
        "                'pitch': note.pitch,\n",
        "                'onset': onset_value,\n",
        "                'duration': duration_value\n",
        "            })\n",
        "    return data\n",
        "\n",
        "def preprocess_midi_folder_to_dict(folder_path):\n",
        "    dataset = {}\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".mid\") or filename.endswith(\".midi\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            try:\n",
        "                midi_data = process_midi_separating_instruments(file_path)\n",
        "                df = pd.DataFrame(midi_data)\n",
        "                midi_file = df['midi_file'].iloc[0]\n",
        "\n",
        "                if midi_file not in dataset:\n",
        "                    dataset[midi_file] = {}\n",
        "\n",
        "                for instrument, group in df.groupby('instrument'):\n",
        "                    if instrument not in dataset[midi_file]:\n",
        "                        dataset[midi_file][instrument] = []\n",
        "\n",
        "                    for row in group.itertuples(index=False):\n",
        "                        onset = Fraction(row.onset) if '/' in str(row.onset) else float(row.onset)\n",
        "                        dataset[midi_file][instrument].append((onset, row.pitch))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing {filename}: {e}\")\n",
        "                continue\n",
        "    return dataset\n",
        "\n",
        "def get_subsets_with_points(preprocessed_dataset):\n",
        "    excluded_tracks = {}\n",
        "\n",
        "    subsets_data = {}\n",
        "    for track_name, instruments_data in preprocessed_dataset.items():\n",
        "        track_filename = track_name.split(\" - \")[-1]\n",
        "        for instrument_name, points in instruments_data.items():\n",
        "            subset_name = f\"{track_filename}_{instrument_name}\"\n",
        "            if subset_name not in excluded_tracks:\n",
        "                subsets_data[subset_name] = points\n",
        "    return subsets_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPaGjqT-H0G8"
      },
      "source": [
        "## To complete by user"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To analyze your own dataset."
      ],
      "metadata": {
        "id": "uQIJMc64dEK4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MR1j7Aakrto_"
      },
      "outputs": [],
      "source": [
        "# 👇 To use:\n",
        "midi_folder_path = \"your_midi_folder_path_here\"\n",
        "preprocessed_dataset = preprocess_midi_folder_to_dict(midi_folder_path)\n",
        "subsets_data = get_subsets_with_points(preprocessed_dataset)\n",
        "\n",
        "print(\"✅ Dataset loaded with\", len(subsets_data), \"subsets.\")\n",
        "\n",
        "ground_truth_df = check_time_signature_presence(midi_folder_path)\n",
        "\n",
        "# 1. Get all unique track names\n",
        "track_names = {\"_\".join(k.split(\"_\")[:-1]) for k in subsets_data}\n",
        "\n",
        "# 2. Convert to sorted list\n",
        "pieces_name = sorted(track_names)\n",
        "print(\"Pieces name:\", pieces_name)\n",
        "\n",
        "# 3. Plot each unique piece\n",
        "for piece in pieces_name:\n",
        "    plot_metric_sections_by_best_compactness(subsets_data, piece, pattern_detection, dilation, ground_truth_df)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "X9y2tLZdNmHx",
        "PNGSbej5s1v3",
        "CmzqcJ96qesX",
        "LNEhvSQsxMgu",
        "mfqmn5rbUe9m",
        "buhvIfu7sUFZ",
        "xdrHN6zx07j6",
        "frHkUmtsrRxu",
        "lHLxEim2-R47",
        "mUiCvXx_LvKW",
        "C0o9an4FSkbk",
        "F4LevAarVjlH",
        "ULCuS0PmQTUO",
        "iIGS1n7kXhSF",
        "qN7qdSXYT816",
        "DW-QlffFldsT",
        "wz6yTdGglgkz",
        "ADvXNP11Zg-V",
        "cyxOF6-ZTyr7",
        "R-ycnkPdTuHU",
        "QWvIWw4hrDWp"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}